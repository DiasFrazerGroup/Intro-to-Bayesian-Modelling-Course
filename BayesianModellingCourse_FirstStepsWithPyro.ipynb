{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiasFrazerGroup/Intro-to-Bayesian-Modelling-Course/blob/main/BayesianModellingCourse_FirstStepsWithPyro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMJoYF1UiV8w"
      },
      "source": [
        "# An Introduction to Bayesian Modelling\n",
        "Jonathan Frazer & Mafalda Dias & Federico Billeci & Charlie Pugh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiCKCgg9VdQf"
      },
      "source": [
        "# **Section 0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEpohevuvQX9"
      },
      "source": [
        "# Probabilistic programming\n",
        "\n",
        "In the coin flipping example, we could compute everything analytically and that is one of the great advantaged of conjugate priors. In more complex models, we rarely have access to this luxury and we must instead proceed with numerical approaches.\n",
        "\n",
        "In this notebook, this is automatically handled by the probabilistic programming language pyro. It is built on pytorch, so can be used in conjunction with modern GPU programming and deep models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TjuP_JRVguD"
      },
      "source": [
        "# **Section 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi7WEfxd21hn"
      },
      "source": [
        "# First steps with Pyro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX42aXtOAn6v"
      },
      "source": [
        "Let's install pyro by evaluating the following line of code. Press the play button on the cell below, or equivalently, type ⌘/Ctrl + Enter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3vI0Vup2zf_",
        "outputId": "83c6b847-d249-49f9-e7f7-73b19ccad2ba"
      },
      "outputs": [],
      "source": [
        "%pip install -q pyro-ppl matplotlib graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cShhdpihCQX8"
      },
      "source": [
        "## Coin flipping revisited (for the 3rd time!)\n",
        "\n",
        "We will start by implementing the coin flipping example from the lecture. Since, as we saw, everything can be computed analytically, we can use this example to explore some aspects of building a model in Pyro and then when we perform parameter inference, we can see how it compares to the analytic result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrRcDxlhBzFs"
      },
      "source": [
        "Import a bunch of libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yaFKaD7Bxcc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.distributions.constraints as constraints\n",
        "import pyro\n",
        "from pyro.optim import Adam\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "import pyro.distributions as dist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# clear the param store in case we're in a REPL\n",
        "pyro.clear_param_store()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSQJ8NugDhL7"
      },
      "source": [
        "Next, we will create some mock data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qNSMCaHDVKC"
      },
      "outputs": [],
      "source": [
        "# create some data with num_tosses tosses of a coin, with bias_factor percentage being heads \n",
        "data = []\n",
        "\n",
        "bias_factor = 0.8\n",
        "num_tosses = 20\n",
        "\n",
        "num_heads = int(bias_factor * num_tosses)\n",
        "num_tails = int((1-bias_factor) * num_tosses)\n",
        "\n",
        "for _ in range(num_heads):\n",
        "    data.append(torch.tensor(1.0))\n",
        "for _ in range(num_tails):\n",
        "    data.append(torch.tensor(0.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRxVGITNDsNW"
      },
      "source": [
        "### Defining the model\n",
        "\n",
        "Next we define the model. The generative story is that\n",
        "* We have a hidden/latent variable, which is the fairness of the coin.\n",
        "  * Recall from the lecture, we can represent our beliefs and uncertainty about this paramater using the Beta distribution.\n",
        "* Our observations are either heads or tails, which we represent as Bernoulli random variables, 0 or 1, respectively.\n",
        "  * We assume that the conditions are not changing from one throw to the next (the iid assumption we discussed in the lecture), hence the fairness parameter is fixed and each throw is independent of the previous throws.\n",
        "\n",
        "Mathematically, we can write this as\n",
        "\\begin{align}\n",
        "\\lambda \\sim& {\\rm Beta}(\\alpha, \\beta) \\\\\n",
        "y_{i} \\sim& {\\rm Bernoulli(\\lambda)}\n",
        "\\end{align}\n",
        "where $i$ labels each throw of the coin and $\\alpha$ and $\\beta$ are the parameters of the Beta distribution that we choose to reflect our beliefs about the coin, prior to observing any data.\n",
        "\n",
        "This is what the model looks like in Pyro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48CxEI6pD2OZ"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters which control the beta prior\n",
        "alpha0 = torch.tensor(5.0)\n",
        "beta0 = torch.tensor(5.0)\n",
        "\n",
        "def model(data):\n",
        "    # sample f from the Beta prior\n",
        "    f = pyro.sample(\"λ\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data\n",
        "    for i in range(len(data)):\n",
        "        # observe datapoint i using the bernoulli likelihood\n",
        "        pyro.sample(\"y_{}\".format(i), dist.Bernoulli(f), obs=data[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vn8XCGvKR2c"
      },
      "source": [
        "You may not be familiar with python, or PyTorch, and so the details of each line might be hard to follow but don't worry. The main point to notice at this stage is that this model definition quite closely resembles the mathematical expression for the model above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyxLfIc6LqC0"
      },
      "source": [
        "Here is what a histogram of samples from the prior looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "_JBYbF4PFFSv",
        "outputId": "c2a8e726-3047-40c9-b3a8-f4165076411b"
      },
      "outputs": [],
      "source": [
        "# Reset plotting parameters\n",
        "plt.rcdefaults()\n",
        "\n",
        "# Number of samples to draw from the prior\n",
        "num_samples = 100000\n",
        "\n",
        "# Sample from the Beta prior\n",
        "prior_samples = pyro.sample(\"prior_samples\", dist.Beta(alpha0, beta0).expand([num_samples]))\n",
        "\n",
        "# Create a histogram to visualize the prior distribution\n",
        "plt.hist(prior_samples, bins=100, density=True, alpha=0.6)\n",
        "plt.title(\"Prior Distribution (Beta)\")\n",
        "plt.xlabel(\"λ\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.xlim(0,1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkNyXt96R0_4"
      },
      "source": [
        "We can also generate a DAG. This can be a handy way to sanity check your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "2zSSciATSB4g",
        "outputId": "17d20d61-08d6-4ebe-de62-9aea95a9487a"
      },
      "outputs": [],
      "source": [
        "pyro.render_model(model, model_args=(data,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMgpMJq1D3XZ"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEz2Hn-qP2vw"
      },
      "source": [
        "#### Inference with MCMC\n",
        "\n",
        "This cleverly takes samples from the posterior wihtout making any assumptions on its functional form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqwgtuduP9oI"
      },
      "outputs": [],
      "source": [
        "from pyro.infer import MCMC, NUTS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your NUTS sampler\n",
        "nuts_kernel = NUTS(model)\n",
        "mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=200)\n",
        "\n",
        "# Run the MCMC sampling\n",
        "mcmc.run(data)\n",
        "posterior_samples = mcmc.get_samples()\n",
        "\n",
        "# Reset plotting parameters\n",
        "plt.rcdefaults()\n",
        "\n",
        "λ_samples = posterior_samples[\"λ\"]\n",
        "plt.hist(λ_samples, bins=30, density=True, alpha=0.6)\n",
        "plt.title(\"Posterior Distribution (MCMC)\")\n",
        "plt.xlabel(\"λ\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.xlim(0,1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sIYtvqtRozZ"
      },
      "source": [
        "#### Inference with VI\n",
        "\n",
        "Now we will repeat this exercise using variational inference instead of VI. The key step is to design a guide. In this example of coin flipping with a $\\rm Beta$ _conjugate_ prior, we therefore know that that the posterior is also given by the $\\rm Beta$ and hence the optimal choice of guide distribution, is also the $\\rm Beta$ distribution, since the goal is to match the posterior as closely as possible. This example is therefore a bit too simple to demonstrate the true power of VI, as the point of VI is to use some simple approximation to a complicated posterior, and not to match it exactly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DziZ590S21Z"
      },
      "outputs": [],
      "source": [
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "# We define the variational distribution or guide. This is the family of distributions in which we search for our posterior distribution. \n",
        "def guide(data):\n",
        "    # register the two variational parameters with Pyro\n",
        "    # - both parameters will have initial value 15.0.\n",
        "    # - because we invoke constraints.positive, the optimizer\n",
        "    # will take gradients on the unconstrained parameters\n",
        "    # (which are related to the constrained parameters by a log)\n",
        "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(10.0),\n",
        "                         constraint=constraints.positive)\n",
        "    beta_q = pyro.param(\"beta_q\", torch.tensor(10.0),\n",
        "                        constraint=constraints.positive)\n",
        "    # sample λ from the distribution Beta(alpha_q, beta_q)\n",
        "    pyro.sample(\"λ\", dist.Beta(alpha_q, beta_q))\n",
        "\n",
        "# setup the optimizer\n",
        "adam_params = {\"lr\": 0.05, \"betas\": (0.90, 0.999)}\n",
        "optimizer = Adam(adam_params)\n",
        "\n",
        "# setup the inference algorithm\n",
        "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
        "# do gradient steps\n",
        "pyro.clear_param_store()\n",
        "n_steps = 2000\n",
        "for step in range(n_steps):\n",
        "    loss = svi.step(data)\n",
        "    if step % 10 == 0:\n",
        "        print('.', end='')\n",
        "# grab the learned variational parameters\n",
        "alpha_q = pyro.param(\"alpha_q\").item()\n",
        "beta_q = pyro.param(\"beta_q\").item()\n",
        "import numpy as np\n",
        "# Generate a range of values for 'λ'\n",
        "λ_values = np.linspace(0, 1, 1000)\n",
        "\n",
        "# Compute the probability density for each value of 'f' based on the variational parameters\n",
        "pdf_values = [dist.Beta(alpha_q, beta_q).log_prob(torch.tensor(λ)).exp().item() for λ in λ_values]\n",
        "\n",
        "# Plot the posterior distribution\n",
        "plt.plot(λ_values, pdf_values)\n",
        "plt.title(\"Posterior Distribution (Variational Inference)\")\n",
        "plt.xlabel(\"λ\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.xlim(0,1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOWGU3RFVz0P"
      },
      "source": [
        "## Exercise 1.1\n",
        "1. List some considerations for choosing a prior in general.\n",
        "\n",
        "2. Compare the prior distribution we plotted above to the posterior distributions inferred. Do you notice any differences? If so, what do these reflect?\n",
        "\n",
        "3. Change the prior hyperparameters in the notebook from the values we defined (`alpha0 = torch.tensor(5.0)`, `beta0 = torch.tensor(5.0)`) and rerun these notebook cells. \n",
        "By changing `alpha0` and `beta0`, try to make a prior that reflects the belief that the coin is definitely fair. Try similarly to make a prior that reflects the belief the coin greatly favours one side. Finally try to make a prior that refelects that we have no information about the fairness of the coin. \n",
        "How do these changes affect the inferred posterior distributions?\n",
        "\n",
        "4. Now try changing the simulated coin tosses that we gave as input data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmCz0-B_T_V_"
      },
      "source": [
        "## Summary/key takeaways from Sections 0 and 1\n",
        "\n",
        "* In practice, most Bayesian modelling requires the use of numerical methods. The conjugate priors we saw in the lecture are the exception not the rule.\n",
        "* Probabilitic programming enables the inference process to be automated. It provides a flexible framework for building both small models, and big deep learning models.\n",
        "* We didn't go into any detail but you should be aware that there are two classes of numerical inference techniques; VI and MCMC.\n",
        "* MCMC is generally more accurate and VI more scalable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glta5W3girTB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ0m5Q1cWSot"
      },
      "source": [
        "# **Section 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRLHlSUE448H"
      },
      "source": [
        "# Experiment Design\n",
        "\n",
        "In this example, we will explore how Bayesian modelling can be used to design experiments which reduce the number of tests one has to make to make in order to reach a conclusion. This is intimately related to topics such as active learning and is a core concept driving many companies seeking to design new drugs, using a combination of high-throughput experiments and deep learning (see, for instance Daphne Koller's company [Insitro](https://insitro.com/), or the startups some of our friends are involved in [Generate Bbiomedicines](https://generatebiomedicines.com/) and [Dyno Therapeutics](https://www.dynotx.com/)).\n",
        "\n",
        "These techniques are less common in basic biology. Perhaps your generation of scientists will be the ones to demonstrate the potential of unified approaches to modelling and experimentation.\n",
        "\n",
        "The main points we want to emphasise are that\n",
        "* Models can be used to make experiments more efficient and/or cheaper.\n",
        "* This is a very active area of research with the potential to transform protein design and possibly many other areas of research.\n",
        "\n",
        "We hope you have fun with the interactive code at the end. If you are too tired to work through this section, feel free to just go through evaluating all the cells and that way you can skip to testing your working memory (which may not be great if you are tired)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqnQFyYN9bYM"
      },
      "source": [
        "### Designing an Adaptive Experiments to Study Your Working Memory\n",
        "\n",
        "This section is based on [this Pyro tutorial](https://pyro.ai/examples/working_memory.html).\n",
        "\n",
        "Bayesian Optimal Experimental Design (BOED) seeks to iteratively design experiments such as to maximise the learning efficiency of a model. When using OED, the data generation and modelling works as follows:\n",
        "\n",
        "1. Write down a Bayesian model involving a design parameter, an unknown latent variable and an observable.\n",
        "\n",
        "2. Choose the optimal design (more details on this later).\n",
        "\n",
        "3. Do the experiment based on the output from 2.\n",
        "\n",
        "4. Collect the results of the experiment done in step 3 and update the model.\n",
        "\n",
        "5. Repeat steps 2, 3 and 4 until the learning process has converged.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVwYQoODA2Yn"
      },
      "source": [
        "#### The experiment set-up\n",
        "\n",
        "Suppose you, the participant, are shown a sequence of digits\n",
        "\n",
        "$$14709$$\n",
        "\n",
        "which are then hidden. You have to to reproduce the sequence exactly from memory. In the next round, the length of the sequence may be different\n",
        "\n",
        "$$23465028063035$$\n",
        "\n",
        "The longest sequence that you can remember is your working memory capacity. In this section, we build a Bayesian model of working memory, and you can then use it to run an adaptive sequence of experiments on yourself to quickly learn your working memory capacity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQKy1MzuCUq2"
      },
      "source": [
        "#### Warmup: A non-adaptive model of working memory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLuWc9AnCziY"
      },
      "outputs": [],
      "source": [
        "%pip install -q scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08WOyr9REteh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pyro\n",
        "import pyro.distributions as dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbhnqNQuEoy7"
      },
      "source": [
        "Our model for a single round of the digits experiment described above has three components: the length $l$\n",
        " of the sequence that the participant has to remember, the participant’s true working memory capacity $\\theta$\n",
        ", and the outcome of the experiment $y$\n",
        " which indicates whether they were able to remember the sequence successfully ($y=1$\n",
        ") or not ($y=0$\n",
        "). We choose a prior for working memory capacity based on [the (in)famous “The magical number seven, plus or minus two”](https://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two).\n",
        "\n",
        "**Note**: $\\theta$\n",
        " actually represents the point where the participant has a 50/50 chance of remembering the sequence correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLBt54776KaQ"
      },
      "outputs": [],
      "source": [
        "# Here we define the hyperparameters of the prior. \n",
        "# This is defining our prior expectation of someones working memory.\n",
        "sensitivity = 1.0\n",
        "prior_mean = torch.tensor(7.0)\n",
        "prior_sd = torch.tensor(3.0)\n",
        "\n",
        "def model(l):\n",
        "    # Dimension -1 of `l` represents the number of rounds\n",
        "    # Other dimensions are batch dimensions: we indicate this with a plate_stack\n",
        "    with pyro.plate_stack(\"plate\", l.shape[:-1]):\n",
        "        theta = pyro.sample(\"theta\", dist.Normal(prior_mean, prior_sd))\n",
        "        # Share theta across the number of rounds of the experiment\n",
        "        # This represents repeatedly testing the same participant\n",
        "        theta = theta.unsqueeze(-1)\n",
        "        # This define a *logistic regression* model for y\n",
        "        logit_p = sensitivity * (theta - l)\n",
        "        # The event shape represents responses from the same participant\n",
        "        y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "8nmGupWkC0NC",
        "outputId": "059e98e4-1add-4534-f168-8020539ca7e0"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.rcParams.update({'font.size': 22})\n",
        "\n",
        "# We sample five times from the prior\n",
        "theta = (prior_mean + prior_sd * torch.randn((5,1)))\n",
        "print(f\"theta values: {theta.T}\")\n",
        "l = torch.arange(1, 16, dtype=torch.float)\n",
        "# This is the same as using 'logits=' in the prior above\n",
        "prob = torch.sigmoid(sensitivity * (theta - l))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for curve in torch.unbind(prob, 0):\n",
        "    plt.plot(l.numpy(), curve.numpy(), marker='o')\n",
        "plt.xlabel(\"Length of sequence $l$\")\n",
        "plt.ylabel(\"Probability of correctly remembering\\na sequence of length $l$\")\n",
        "plt.legend([\"Person {}\".format(i+1) for i in range(5)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7MwPl3NR_-h"
      },
      "source": [
        "#### Exercise 2.1\n",
        "1. Can you think of any ways to improve the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3m_3k_7Fh58"
      },
      "source": [
        "To understand the relationship between $\\theta$ and $l$, consider following plots. The probability of successfully remembering the sequence is plotted below, for five random samples of $\\theta$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEjm54JDGBfr"
      },
      "source": [
        "#### Inference in the model\n",
        "\n",
        "As in the coin flipping example, let's perform variational inference using VI on some mock data. This time we define a Normal guide for variational inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG3RKofLC2oZ"
      },
      "outputs": [],
      "source": [
        "from torch.distributions.constraints import positive\n",
        "\n",
        "def guide(l):\n",
        "    # The guide is initialised at the prior:\n",
        "\n",
        "    # Create a PyTorch tensor named \"posterior_mean\" and register it as a Pyro parameter.\n",
        "    # Initialize it with the value from \"prior_mean\" (which should be defined elsewhere).\n",
        "    posterior_mean = pyro.param(\"posterior_mean\", prior_mean.clone())\n",
        "\n",
        "    # Create another PyTorch tensor named \"posterior_sd\" and register it as a Pyro parameter.\n",
        "    # Initialize it with the value from \"prior_sd\" (which should be defined elsewhere).\n",
        "    # The \"constraint=positive\" argument ensures that \"posterior_sd\" remains positive.\n",
        "    posterior_sd = pyro.param(\"posterior_sd\", prior_sd.clone(), constraint=positive)\n",
        "\n",
        "    # Sample \"theta\" from a Normal distribution.\n",
        "    # The distribution is centered at \"posterior_mean\" and has a standard deviation of \"posterior_sd\".\n",
        "    pyro.sample(\"theta\", dist.Normal(posterior_mean, posterior_sd))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF048XjiHiei"
      },
      "source": [
        "We finally specify the following data: the participant was shown sequences of lengths 5, 7 and 9. They remembered the first two correctly, but not the third one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4gZpZHaC6DI"
      },
      "outputs": [],
      "source": [
        "l_data = torch.tensor([5., 7., 9.])\n",
        "y_data = torch.tensor([1., 1., 0.])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufXsP76FH6QN"
      },
      "source": [
        "We can now run SVI on the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYR3cClDC8Ta",
        "outputId": "f56344e0-47a4-4a79-8452-f094f023119d"
      },
      "outputs": [],
      "source": [
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "# Condition the model on observed data 'y_data'\n",
        "conditioned_model = pyro.condition(model, {\"y\": y_data})\n",
        "\n",
        "# Create an instance of SVI\n",
        "# - `conditioned_model`: The model with observed data\n",
        "# - `guide`: The guide function, which serves as a variational distribution\n",
        "# - Adam optimizer with a learning rate of 0.001\n",
        "# - Use Trace_ELBO as the loss function\n",
        "# - Perform 100 samples per iteration\n",
        "svi = SVI(conditioned_model, guide, Adam({\"lr\": .001}), loss=Trace_ELBO(), num_samples=100)\n",
        "\n",
        "# Clear the Pyro parameter store to start with a clean slate\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# Number of iterations for SVI\n",
        "num_iters = 5000\n",
        "\n",
        "# Perform SVI iterations\n",
        "for i in range(num_iters):\n",
        "    # Perform one SVI step and calculate the Evidence Lower Bound (ELBO)\n",
        "    elbo = svi.step(l_data)\n",
        "\n",
        "    # Print the negative ELBO every 500 iterations (optional)\n",
        "    if i % 500 == 0:\n",
        "        print(\"Neg ELBO:\", elbo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNllLEV4C-iq",
        "outputId": "00f48790-f582-4a81-a651-82f49fdf1f1c"
      },
      "outputs": [],
      "source": [
        "print(\"Prior:     N({:.3f}, {:.3f})\".format(prior_mean, prior_sd))\n",
        "print(\"Posterior: N({:.3f}, {:.3f})\".format(pyro.param(\"posterior_mean\"),\n",
        "                                            pyro.param(\"posterior_sd\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Vio-24Ijt3"
      },
      "source": [
        "So we see that after these three observations, the posterior has deviated from the prior and we now have a sharper estimate of this person's working memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfskzkdaJCYK"
      },
      "source": [
        "#### Bayesian optimal experimental design¶\n",
        "\n",
        "The problem with above approach is that the experimenter needs to choose the values for $l$. The goal of BOED (Bayesian optimal experimental design) in this case is to find the best choice of $l$, given the experiments done up to that point.\n",
        "\n",
        "##### Intuition\n",
        "\n",
        "We want to maximise the amount of information we learn at each test we provide (Information Gain / IG). This will mean we either need less tests or get a better estimate at the end of $\\theta$\n",
        "\n",
        "But for a new test length l we do not know whether the participant will answer correctly or not yet so therefore we must look at the expected amount of information gained (EIG) whether they answer correctly or not. We will use our current best model to calculate this expectation. So - given our understanding of the participants memory so far, what new $l$ we can test them with do we expect to be the most informative.  \n",
        "\n",
        "##### Mathematical details\n",
        "**WARNING**: This next bit gets quite technical. You might want to go and get a coffee first. Or perhaps leave it for another day and skip to the next subsection [BOED Implementation in Pyro](#BOED-Implementation-in-Pyro).\n",
        "\n",
        "What do we mean \"best choice of $l$\"? In this case, we want to find the best estimate for $\\theta$ (or equivalently $l$) in the minimum number of experiments. That is we want to maximise the information with each new measurement. We can quantify this with a measure of the \"difference\" between posterior $p(\\theta|y, l)$ and the prior $p(\\theta)$ using the Kullback-Leibler divergence, defined as\n",
        "$$D_{\\rm KL}(p(x)||q(x)) = \\int p(x)\\log\\left(\\frac{p(x)}{q(x)} \\right)dx.$$\n",
        "This is a special case the more general class of divergences, and is a key quantity in the amazing field of information geometry. This is beyond the scope of today's class though. For the purposes of today, you can think of it as a measure of the difference between the two distributions. So in our case we have\n",
        "$${\\rm IG}(l,y) = KL(p(\\theta|y,l)||p(\\theta))$$\n",
        "and with each experiment, we want the posterior to become as different as possible from the prior, as measured by ${\\rm IG}(l,y)$.\n",
        "\n",
        "Unfortunately, we will not know $y$\n",
        " until we actually run the experiment. Therefore, the next best option would be to choose $l$\n",
        " on the basis of the _expected_ information gain\n",
        "$${\\rm EIG}(l) = \\mathbb{E}_{y\\sim p(y|\\theta, l)}\\left [KL(p(\\theta|y,l)||p(\\theta))\\right ]$$\n",
        "...Unfortunately, this is intractable due to requiring the posterior $p(\\theta|y, l)$. [This NeurIPS paper from 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/d55cbf210f175f4a37916eafe6c04f0d-Paper.pdf) involving a bunch of machine learning rock stars suggests using the following estimator\n",
        "$${\\rm EIG}(l) = \\min_q\\mathbb{E}_{\\theta, y\\sim p(\\theta)p(y|\\theta, l)}\\left [\\log\\left (\\frac{p(y|\\theta, l)}{q(y|l)}\\right)\\right].$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJJQN3cEPh9-"
      },
      "source": [
        "#### BOED Implementation in Pyro\n",
        "Fortunately, Pyro comes ready with tools to estimate the EIG. All we have to do is define the “marginal guide” $q(y|l)$\n",
        " in the formula above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6ubGycwDzRO"
      },
      "outputs": [],
      "source": [
        "def marginal_guide(design, observation_labels, target_labels):\n",
        "    # This shape allows us to learn a different parameter for each candidate design l\n",
        "    q_logit = pyro.param(\"q_logit\", torch.zeros(design.shape[-2:]))\n",
        "    pyro.sample(\"y\", dist.Bernoulli(logits=q_logit).to_event(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIggtBA5Dz-j"
      },
      "outputs": [],
      "source": [
        "from pyro.contrib.oed.eig import marginal_eig\n",
        "\n",
        "# The shape of `candidate_designs` is (number designs, 1)\n",
        "# This represents a batch of candidate designs, each design is for one round of experiment\n",
        "candidate_designs = torch.arange(1, 15, dtype=torch.float).unsqueeze(-1)\n",
        "pyro.clear_param_store()\n",
        "num_steps, start_lr, end_lr = 1000, 0.1, 0.001\n",
        "optimizer = pyro.optim.ExponentialLR({'optimizer': torch.optim.Adam,\n",
        "                                      'optim_args': {'lr': start_lr},\n",
        "                                      'gamma': (end_lr / start_lr) ** (1 / num_steps)})\n",
        "\n",
        "eig = marginal_eig(model,\n",
        "                   candidate_designs,       # design, or in this case, tensor of possible designs\n",
        "                   \"y\",                     # site label of observations, could be a list\n",
        "                   \"theta\",                 # site label of 'targets' (latent variables), could also be list\n",
        "                   num_samples=100,         # number of samples to draw per step in the expectation\n",
        "                   num_steps=num_steps,     # number of gradient steps\n",
        "                   guide=marginal_guide,    # guide q(y)\n",
        "                   optim=optimizer,         # optimizer with learning rate decay\n",
        "                   final_num_samples=10000  # at the last step, we draw more samples\n",
        "                                            # for a more accurate EIG estimate\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0G_SlMWTyKv"
      },
      "source": [
        "We can visualize the EIG estimates that we found, *the expected information gained given experiments with different lengths asked*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "VpVbmUknD1ru",
        "outputId": "4a73e1f0-b30b-4e14-941f-244ecaabbccb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "matplotlib.rcParams.update({'font.size': 22})\n",
        "plt.plot(candidate_designs.numpy(), eig.detach().numpy(), marker='o', linewidth=2)\n",
        "plt.xlabel(\"$l$\")\n",
        "plt.ylabel(\"EIG($l$)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J347YXxhD3UU",
        "outputId": "5edb70f2-a28c-4135-a67a-27bd7ff30cc4"
      },
      "outputs": [],
      "source": [
        "best_l = 1 + torch.argmax(eig)\n",
        "print(\"Optimal design:\", best_l.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLxftL5sUDNq"
      },
      "source": [
        "This tells us that the first round should be run with a sequence of length 7. Note that, while we might have been able to guess this optimal design intuitively, this same framework applies equally well to more sophisticated models and experiments where finding the optimal design by intuition is more challenging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpYAFYziURsf"
      },
      "source": [
        "#### The adaptive experiment\n",
        "\n",
        "We now have the ingredients to build an adaptive experiment to study working memory.\n",
        "\n",
        "At the first iteration, step 1 is done using the prior as above. However, for subsequent iterations, we use the posterior given all the data so far.\n",
        "\n",
        "In this notebook, the “experiment” is performed using the following synthesiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzbnz3gCD9NB"
      },
      "outputs": [],
      "source": [
        "def synthetic_person(l):\n",
        "    # The synthetic person can remember any sequence shorter than 6\n",
        "    # They cannot remember any sequence of length 6 or above\n",
        "    # (There is no randomness in their responses)\n",
        "    y = (l < 6.).float()\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYlhN-dUD_S_"
      },
      "outputs": [],
      "source": [
        "def make_model(mean, sd):\n",
        "    def model(l):\n",
        "        # Dimension -1 of `l` represents the number of rounds\n",
        "        # Other dimensions are batch dimensions: we indicate this with a plate_stack\n",
        "        with pyro.plate_stack(\"plate\", l.shape[:-1]):\n",
        "            theta = pyro.sample(\"theta\", dist.Normal(mean, sd))\n",
        "            # Share theta across the number of rounds of the experiment\n",
        "            # This represents repeatedly testing the same participant\n",
        "            theta = theta.unsqueeze(-1)\n",
        "            # This define a *logistic regression* model for y\n",
        "            logit_p = sensitivity * (theta - l)\n",
        "            # The event shape represents responses from the same participant\n",
        "            y = pyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))\n",
        "            return y\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-EHgf5eEBO6",
        "outputId": "746a45eb-09f1-4e33-93a8-7b9f631037bc"
      },
      "outputs": [],
      "source": [
        "ys = torch.tensor([])\n",
        "ls = torch.tensor([])\n",
        "history = [(prior_mean, prior_sd)]\n",
        "pyro.clear_param_store()\n",
        "current_model = make_model(prior_mean, prior_sd)\n",
        "\n",
        "eigs = []\n",
        "for experiment in range(10):\n",
        "    print(\"Round\", experiment + 1)\n",
        "\n",
        "    # Step 1: compute the optimal length\n",
        "    optimizer = pyro.optim.ExponentialLR({'optimizer': torch.optim.Adam,\n",
        "                                          'optim_args': {'lr': start_lr},\n",
        "                                          'gamma': (end_lr / start_lr) ** (1 / num_steps)})\n",
        "    eig = marginal_eig(current_model, candidate_designs, \"y\", \"theta\", num_samples=100,\n",
        "                       num_steps=num_steps, guide=marginal_guide, optim=optimizer,\n",
        "                       final_num_samples=10000)\n",
        "    best_l = 1 + torch.argmax(eig).float().detach()\n",
        "\n",
        "    eigs.append(eig)\n",
        "\n",
        "    # Step 2: run the experiment, here using the synthetic person\n",
        "    print(\"Asking the participant to remember a sequence of length\", int(best_l.item()))\n",
        "    y = synthetic_person(best_l)\n",
        "    if y:\n",
        "        print(\"Participant remembered correctly\")\n",
        "    else:\n",
        "        print(\"Participant could not remember the sequence\")\n",
        "    # Store the sequence length and outcome\n",
        "    ls = torch.cat([ls, best_l.expand(1)], dim=0)\n",
        "    ys = torch.cat([ys, y.expand(1)])\n",
        "\n",
        "    # Step 3: learn the posterior using all data seen so far\n",
        "    conditioned_model = pyro.condition(model, {\"y\": ys})\n",
        "    svi = SVI(conditioned_model,\n",
        "              guide,\n",
        "              Adam({\"lr\": .005}),\n",
        "              loss=Trace_ELBO(),\n",
        "              num_samples=100)\n",
        "    num_iters = 2000\n",
        "    for i in range(num_iters):\n",
        "        elbo = svi.step(ls)\n",
        "\n",
        "    history.append((pyro.param(\"posterior_mean\").detach().clone().numpy(),\n",
        "                    pyro.param(\"posterior_sd\").detach().clone().numpy()))\n",
        "    current_model = make_model(pyro.param(\"posterior_mean\").detach().clone(),\n",
        "                               pyro.param(\"posterior_sd\").detach().clone())\n",
        "    print(\"Estimate of \\u03b8: {:.3f} \\u00b1 {:.3f}\\n\".format(*history[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yRpMeYOTOvc"
      },
      "source": [
        "Now let’s visualize the evolution of the posterior for $\\theta$ (Blue = prior, light green = 10 step posterior):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.cm as cmx\n",
        "\n",
        "\n",
        "matplotlib.rcParams.update({'font.size': 22})\n",
        "cmap = plt.get_cmap('winter')\n",
        "cNorm  = colors.Normalize(vmin=0, vmax=len(history)-1)\n",
        "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
        "plt.figure(figsize=(12, 6))\n",
        "x = np.linspace(0, 14, 100)\n",
        "for idx, (mean, sd) in enumerate(history):\n",
        "    color = scalarMap.to_rgba(idx)\n",
        "    y = norm.pdf(x, mean, sd)\n",
        "    plt.plot(x, y, color=color)\n",
        "    plt.xlabel(\"$\\\\theta$\")\n",
        "    plt.ylabel(\"p.d.f.\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And lets look at the EIG (expected information gain) of each of the 10 experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matplotlib.rcParams.update({'font.size': 22})\n",
        "cmap = plt.get_cmap('winter')\n",
        "cNorm  = colors.Normalize(vmin=0, vmax=len(history)-1)\n",
        "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for idx, eig in enumerate(eigs):\n",
        "    color = scalarMap.to_rgba(idx)\n",
        "    l_chosen = eig.argmax().item() + 1\n",
        "    plt.scatter(idx+1, l_chosen, color=color)\n",
        "    plt.ylabel(\"$l$ chosen (providing maximal EIG)\")\n",
        "    plt.xlabel(\"Iteration number\")\n",
        "\n",
        "plt.axhline(6, linestyle='--', color='red')\n",
        "plt.ylim(0,14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR643nnJUL5f"
      },
      "source": [
        "By contrast, suppose we use a simplistic design: try sequences of lengths 1, 2, …, 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNrElYFMUOef"
      },
      "outputs": [],
      "source": [
        "pyro.clear_param_store()\n",
        "ls = torch.arange(1, 11, dtype=torch.float)\n",
        "ys = synthetic_person(ls)\n",
        "conditioned_model = pyro.condition(model, {\"y\": ys})\n",
        "svi = SVI(conditioned_model,\n",
        "          guide,\n",
        "          Adam({\"lr\": .005}),\n",
        "          loss=Trace_ELBO(),\n",
        "          )\n",
        "num_iters = 2000\n",
        "for i in range(num_iters):\n",
        "    elbo = svi.step(ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "4LY5QTdDUQQC",
        "outputId": "0e913357-4b90-46bf-bf0e-b5814d918c3f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "matplotlib.rcParams.update({'font.size': 22})\n",
        "y1 = norm.pdf(x, pyro.param(\"posterior_mean\").detach().numpy(),\n",
        "              pyro.param(\"posterior_sd\").detach().numpy())\n",
        "y2 = norm.pdf(x, history[-1][0], history[-1][1])\n",
        "plt.plot(x, y1)\n",
        "plt.plot(x, y2)\n",
        "plt.legend([\"Simple design\", \"Optimal design\"])\n",
        "plt.xlabel(\"$\\\\theta$\")\n",
        "plt.ylabel(\"p.d.f.\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHBC9sUEUUcH"
      },
      "source": [
        "Although both design strategies give us data, the optimal strategy ends up with a posterior distribution that is more peaked: that means we have greater confidence in our final answer, or may be able to stop experimenting earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxnlONo6hbBT"
      },
      "source": [
        "#### Exercise 2.2\n",
        "1. Test your working memory by using the following code.\n",
        "\n",
        "\n",
        "\n",
        "2. Change the values below defining the prior expectation of the working memory of the population, `prior_mean = torch.tensor(7.0)`, `prior_sd = torch.tensor(3.0)`. Observe how this changes the behavior of the next chosen length based on EIG (expected information gain). Try making very good or very bad guesses to make the effects more apparent. \n",
        "\n",
        "\n",
        "**Notes**:\n",
        "* Running this requires first evaluating the cells above in this section.\n",
        "* Keep the values reasonable (roughly less than 20 greater than 0) for numerical reasons. \n",
        "* To simplify things, the maximum possible value we have hardcoded is 14. You may notice this in the behaviour of the predictions. \n",
        "* You can try to cheat!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wRCBIz5jAmV"
      },
      "source": [
        "The code below is the same as above except for small modifications to enable the synthetic_person to be replaced by a real person."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is defining our prior expectation of someones working memory. Note it modelled as Gaussian.\n",
        "\n",
        "# This is the mean of the 'working memory' across people\n",
        "prior_mean = torch.tensor(7.0)\n",
        "\n",
        "# This is the standard devation\n",
        "prior_sd = torch.tensor(3.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from IPython.display import display, update_display, HTML\n",
        "import torch\n",
        "\n",
        "def generate_random_integer(length):\n",
        "    if length <= 0:\n",
        "        raise ValueError(\"Length must be greater than 0\")\n",
        "\n",
        "    # Define the range for the random integer\n",
        "    lower_bound = 10 ** (length - 1)\n",
        "    upper_bound = 10 ** length - 1\n",
        "\n",
        "    # Sample a random integer within the specified range\n",
        "    random_int = torch.randint(lower_bound, upper_bound + 1, (1,))\n",
        "\n",
        "    return random_int.item()\n",
        "\n",
        "def show_number_for_time_jupyter(number, display_time):\n",
        "    # Display the number without quotes\n",
        "    display(HTML(f\"Remember this number: {number}\"), display_id='a')\n",
        "    \n",
        "    # Pause for the specified time (in seconds)\n",
        "    time.sleep(display_time)\n",
        "\n",
        "    # Overwrite the number to \"erase\" it, again without quotes\n",
        "    update_display(HTML(f\"Remember this number: {''.join(['*']*len(str(number)))}\"), display_id='a')  \n",
        "\n",
        "def ask_user_for_number_jupyter():\n",
        "    # Ask the user what the number was\n",
        "    return input(\"What was the number? \")\n",
        "\n",
        "def number_quiz(number_to_show, display_time=7):\n",
        "    # Show the number and then hide it\n",
        "    show_number_for_time_jupyter(number_to_show, display_time)\n",
        "    # Ask the user and check their answer\n",
        "    user_guess = ask_user_for_number_jupyter()\n",
        "    return str(user_guess) == str(number_to_show)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X9s0TeRbbIg",
        "outputId": "6e72cdda-ecfc-4bf9-d23a-1fd364135f96"
      },
      "outputs": [],
      "source": [
        "ys = torch.tensor([])\n",
        "ls = torch.tensor([])\n",
        "history = [(prior_mean, prior_sd)]\n",
        "pyro.clear_param_store()\n",
        "current_model = make_model(prior_mean, prior_sd)\n",
        "\n",
        "eigs = []\n",
        "for experiment in range(10):\n",
        "    print(\"Round\", experiment + 1)\n",
        "\n",
        "    # Step 1: compute the optimal length\n",
        "    optimizer = pyro.optim.ExponentialLR({'optimizer': torch.optim.Adam,\n",
        "                                          'optim_args': {'lr': start_lr},\n",
        "                                          'gamma': (end_lr / start_lr) ** (1 / num_steps)})\n",
        "    eig = marginal_eig(current_model, candidate_designs, \"y\", \"theta\", num_samples=100,\n",
        "                       num_steps=num_steps, guide=marginal_guide, optim=optimizer,\n",
        "                       final_num_samples=10000)\n",
        "    best_l = 1 + torch.argmax(eig).float().detach()\n",
        "\n",
        "    eigs.append(eig)\n",
        "\n",
        "    # Step 2: run the experiment, using user input\n",
        "    # Generate random integer of best length to maximise expected information gain \n",
        "    random_integer = generate_random_integer(int(best_l.item()))\n",
        "    # Test user\n",
        "    quiz_result = number_quiz(random_integer)\n",
        "    \n",
        "    y = torch.tensor([float(quiz_result)])\n",
        "    if y:\n",
        "        print(\"Participant remembered correctly\")\n",
        "    else:\n",
        "        print(\"Participant could not remember the sequence\")\n",
        "    # Store the sequence length and outcome\n",
        "    ls = torch.cat([ls, best_l.expand(1)], dim=0)\n",
        "    ys = torch.cat([ys, y.expand(1)])\n",
        "\n",
        "    # Step 3: learn the posterior using all data seen so far\n",
        "    conditioned_model = pyro.condition(model, {\"y\": ys})\n",
        "    svi = SVI(conditioned_model,\n",
        "              guide,\n",
        "              Adam({\"lr\": .005}),\n",
        "              loss=Trace_ELBO(),\n",
        "              num_samples=100)\n",
        "    num_iters = 2000\n",
        "    for i in range(num_iters):\n",
        "        elbo = svi.step(ls)\n",
        "\n",
        "    history.append((pyro.param(\"posterior_mean\").detach().clone().numpy(),\n",
        "                    pyro.param(\"posterior_sd\").detach().clone().numpy()))\n",
        "    current_model = make_model(pyro.param(\"posterior_mean\").detach().clone(),\n",
        "                               pyro.param(\"posterior_sd\").detach().clone())\n",
        "    print(\"Estimate of \\u03b8: {:.3f} \\u00b1 {:.3f}\\n\".format(*history[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "9GgFvUexgHKH",
        "outputId": "a14c1f86-6169-47ce-d63c-7d77f4e7389b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.cm as cmx\n",
        "\n",
        "\n",
        "matplotlib.rcParams.update({'font.size': 22})\n",
        "cmap = plt.get_cmap('winter')\n",
        "cNorm  = colors.Normalize(vmin=0, vmax=len(history)-1)\n",
        "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
        "plt.figure(figsize=(12, 6))\n",
        "x = np.linspace(0, 14, 100)\n",
        "for idx, (mean, sd) in enumerate(history):\n",
        "    color = scalarMap.to_rgba(idx)\n",
        "    y = norm.pdf(x, mean, sd)\n",
        "    plt.plot(x, y, color=color)\n",
        "    plt.xlabel(\"$\\\\theta$\")\n",
        "    plt.ylabel(\"p.d.f.\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matplotlib.rcParams.update({'font.size': 22})\n",
        "cmap = plt.get_cmap('winter')\n",
        "cNorm  = colors.Normalize(vmin=0, vmax=len(history)-1)\n",
        "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for idx, eig in enumerate(eigs):\n",
        "    color = scalarMap.to_rgba(idx)\n",
        "    l_chosen = eig.argmax().item() + 1\n",
        "    plt.scatter(idx+1, l_chosen, color=color)\n",
        "    plt.ylabel(\"$l$ chosen (providing maximal EIG)\")\n",
        "    plt.xlabel(\"Iteration number\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDP1iehkoL94"
      },
      "source": [
        "## Summary of Section 2: Experiment Design\n",
        "\n",
        "The main points we want to emphasise about this section are\n",
        "* Models can be used to make experiments more efficient and/or cheaper.\n",
        "* This is a very active area of research with the potential to transform protein design and possibly many other areas of research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMoOPWPPixGx"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Section 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Towards more complex models\n",
        "\n",
        "If you have made it this far, then congratulations! You have covered a lot of ground today.\n",
        "\n",
        "A beautiful aspect of Bayesian modelling, combined with probabilistic programming, is that we have a unified framework for working with both simple models, such as the coin flipping example, and more complex models, such as those needed to analyse and design experiments, or those encountered in the context of deep learning. In this section we will start to dabble in more complex problems but first, we need to introduce two more concept -- the use of plates and hierarchical models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plate notation\n",
        "\n",
        "Representing each observation as a separate node is a bit inefficient. Going back to the coin example above we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pyro.clear_param_store()\n",
        "\n",
        "def model(data):\n",
        "    # define the hyperparameters that control the Beta prior\n",
        "    alpha0 = torch.tensor(10.0)\n",
        "    beta0 = torch.tensor(10.0)\n",
        "    # sample f from the Beta prior\n",
        "    f = pyro.sample(\"λ\", dist.Beta(alpha0, beta0))\n",
        "    # loop over the observed data\n",
        "    for i in range(len(data)):\n",
        "        # observe datapoint i using the bernoulli likelihood\n",
        "        pyro.sample(\"y_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
        "        \n",
        "pyro.render_model(model, model_args=(data,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since it is common to have many variables that are independent and identically distributed, there is a special notation used in order to simplify the DAG, and Pyro code, which is called a plate. An equivalent way of representing this model is as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_v2(data):\n",
        "  # define the hyperparameters that control the Beta prior\n",
        "  alpha0 = torch.tensor(10.0)\n",
        "  beta0 = torch.tensor(10.0)\n",
        "  # sample f from the Beta prior\n",
        "  f = pyro.sample(\"λ\", dist.Beta(alpha0, beta0))\n",
        "  # loop over the observed data\n",
        "\n",
        "  # for i in range(len(data)):\n",
        "  #     # observe datapoint i using the bernoulli likelihood\n",
        "  #     pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
        "\n",
        "  #Replace the loop we had previously with a plate\n",
        "  with pyro.plate(\"N\", len(data)):\n",
        "    pyro.sample(\"y\", dist.Bernoulli(f), obs=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and now the DAG looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pyro.render_model(model_v2, model_args=(torch.tensor(data),))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where $N$ represents the number of observations. So in this case $N = 10$. In Pyro plates can be used to informs inference algorithms that the variables being indexed are conditionally independent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hierarchical Modelling\n",
        "\n",
        "The use of priors in Bayesian modelling, the use of probabilistic programming, the use of DAGs as a way to visualise models,...everything we have discuessed thus far, all start to show their true value when we consider models with a rich structure of dependencies between variables.\n",
        "\n",
        "Hierarchical modeling is used to model complex data structures that exhibit hierarchy or nested structure. It's particularly useful when you have data that can be naturally organized into groups or levels, and you want to estimate parameters at each level while accounting for the variability both within and between groups.\n",
        "\n",
        "Another way to think about hierarchical modelling is as an information sharing mechanism between subgroups. i.e. You might have collections of data that are distinct but have some common properties. In this case you can use hierarchical modelling as a way to learn those common features, while also providing the freedom for each group to differ. It can also be a very effective way of avoiding certain types of overfitting, since hierarchical structure can be thought of as learning an informative prior for each subgroup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicting the Progresson of Pulmonary Fibrosis with Bayesian Hierarchical Linear Regression\n",
        "\n",
        "This section is adapted from [the tutorial by Carlos Souza](https://num.pyro.ai/en/stable/tutorials/bayesian_hierarchical_linear_regression.html).\n",
        "\n",
        "Here we use NumPyro. It is less mature than the Pyro, but is blazing fast for MCMC computations. It uses JAX, rather than PyTorch as a backend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Understanding the task\n",
        "Pulmonary fibrosis is a disorder with no known cause and no known cure, created by scarring of the lungs. The goal of this section is to predict a patient’s severity of decline in lung function. Lung function is assessed based on output from a spirometer, which measures the forced vital capacity (FVC), i.e. the volume of air exhaled.\n",
        "\n",
        "Let's explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q numpyro arviz seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpyro\n",
        "from numpyro.infer import MCMC, NUTS, Predictive\n",
        "import numpyro.distributions as dist\n",
        "from jax import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = pd.read_csv(\n",
        "    \"https://gist.githubusercontent.com/ucals/\"\n",
        "    \"2cf9d101992cb1b78c2cdd6e3bac6a4b/raw/\"\n",
        "    \"43034c39052dcf97d4b894d2ec1bc3f90f3623d9/\"\n",
        "    \"osic_pulmonary_fibrosis.csv\"\n",
        ")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the dataset, we were provided with a baseline chest CT scan and associated clinical information for a set of patients. A patient has an image acquired at time Week = 0 and has numerous follow up visits over the course of approximately 1-2 years, at which time their FVC is measured. For this example, we will use only the Patient ID, the weeks and the FVC measurements, discarding all the rest.\n",
        "\n",
        "Since this is real medical data, the relative timing of FVC measurements varies widely, as shown in the 3 sample patients below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset plotting parameters\n",
        "plt.rcdefaults()\n",
        "sns.reset_defaults()\n",
        "\n",
        "def chart_patient(patient_id, ax):\n",
        "    data = train[train[\"Patient\"] == patient_id]\n",
        "    x = data[\"Weeks\"]\n",
        "    y = data[\"FVC\"]\n",
        "    ax.set_title(patient_id)\n",
        "    sns.regplot(x=x, y=y, ax=ax, ci=None, line_kws={\"color\": \"red\"})\n",
        "\n",
        "f, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "chart_patient(\"ID00007637202177411956430\", axes[0])\n",
        "chart_patient(\"ID00009637202177434476278\", axes[1])\n",
        "chart_patient(\"ID00010637202177584971671\", axes[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On average, each of the 176 provided patients made 9 visits, when FVC was measured. The visits happened in specific weeks in the [-12, 133] interval. The decline in lung capacity is very clear. We see, though, they are very different from patient to patient. We will predict every patient's FVC measurement for every possible week in the [-12, 133] interval, and the confidence for each prediction.\n",
        "\n",
        "Before moving on to the next section try to complete the following exercise\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise 3.1\n",
        "Before we make things more complicated, first let's imagine we have just one patient. We assume a linear relationship between FVC and time $t$\n",
        "$${\\rm FVC} = \\beta t + \\alpha$$\n",
        "1. The above expression is missing the fact that our data is \"noisy\". Add a term to the above expression to account for noise.\n",
        "2. Improve your model from 1 by turning it into a fully Bayesian generative story.\n",
        "3. Draw the DAG for your model. Feel free to just draw it by hand but if you are feeling more ambitious, try coding up the model and using numpyro to render the dag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answers to Exercise 3.1\n",
        "1. The above expression is missing the fact that our data is \"noisy\". Add a term to the above expression to account for noise.\n",
        "\n",
        "**Answer**: A natural extension would be to add an explicit noise term\n",
        "$${\\rm FVC} = \\beta t + \\alpha + \\epsilon$$\n",
        "where $\\epsilon$ could come from a centred Gaussian with standard deviation $\\sigma$\n",
        "$$\\epsilon \\sim N(0,\\sigma),$$\n",
        "\n",
        "**Note**:\n",
        "An equivalent way of writing this is\n",
        "$${\\rm FVC} \\sim N(\\beta t + \\alpha,\\sigma)$$\n",
        "\n",
        "2. Improve your model from 1 by turning it into a fully Bayesian generative story.\n",
        "\n",
        "**Answer**: We could try something like the following\n",
        "\\begin{align}\n",
        "\\alpha &\\sim \\text{Normal}(\\mu_{\\alpha}, \\sigma_{\\alpha}) \\\\\n",
        "\\beta &\\sim \\text{Normal}(\\mu_{\\beta}, \\sigma_{\\beta}) \\\\\n",
        "\\sigma &\\sim \\text{Half-Normal}(100) \\\\\n",
        "FVC_{ij} &\\sim \\text{Normal}(\\alpha + t \\beta, \\sigma)\n",
        "\\end{align}\n",
        "\n",
        "**Note**:\n",
        "* The precise choice of prior does not matter. The point is just to write down a complete generative model.\n",
        "* You just reinvented/rediscovered Bayesian linear regression.   \n",
        "\n",
        "3. Draw the DAG for your model. Feel free to just draw it by hand but if you are feeling more ambitious, try coding up the model and using numpyro to render the dag.\n",
        "\n",
        "**Answer**: Its the same as the lower part of the diagram below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A really over the top way of making the DAG is to code up the model.\n",
        "\n",
        "def bayesian_linear_regression_model(x, y):\n",
        "    # Priors for the regression coefficients\n",
        "    beta = numpyro.sample(\"beta\", dist.Normal(0, 1))\n",
        "    alpha = numpyro.sample(\"alpha\", dist.Normal(0, 1))\n",
        "\n",
        "    # Variance for the observations\n",
        "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(1))\n",
        "\n",
        "    # Linear regression model\n",
        "    with numpyro.plate(\"data\", len(x)):\n",
        "        y_mean = beta * x + alpha\n",
        "        numpyro.sample(\"obs\", dist.Normal(y_mean, sigma), obs=y)\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "x = np.random.rand(100)\n",
        "y = 2 * x + 1 + 0.1 * np.random.randn(100)\n",
        "\n",
        "# Render the model using numpyro.render_model\n",
        "numpyro.render_model(bayesian_linear_regression_model, model_args=(x, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modelling: Bayesian Hierarchical Linear Regression with Partial Pooling\n",
        "The simplest possible linear regression, not hierarchical, would assume all FVC decline curves have the same $\\alpha$ and $\\beta$. That's the **pooled model**. In the other extreme, we could assume a model where each patient has a personalized FVC decline curve, and **these curves are completely unrelated**. That's the **unpooled model**, where each patient has completely separate regressions.\n",
        "\n",
        "Here, we go for the middle ground: **Partial pooling**. Specifically, we assume that while $\\alpha$'s and $\\beta$'s are different for each patient as in the unpooled case, **the coefficients all share similarity**. We can model this by assuming that each individual coefficient comes from a common group distribution. The image below represents this model graphically:\n",
        "\n",
        "<img src=\"https://i.ibb.co/H7NgBfR/Artboard-2-2x-100.jpg\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "The model has the following generative story:\n",
        "\n",
        "\\begin{align}\n",
        "\\mu_{\\alpha} &\\sim \\text{Normal}(0, 500) \\\\\n",
        "\\sigma_{\\alpha} &\\sim \\text{Half-Normal}(100) \\\\\n",
        "\\mu_{\\beta} &\\sim \\text{Normal}(0, 3) \\\\\n",
        "\\sigma_{\\beta} &\\sim \\text{Half-Normal}(3) \\\\\n",
        "\\alpha_i &\\sim \\text{Normal}(\\mu_{\\alpha}, \\sigma_{\\alpha}) \\\\\n",
        "\\beta_i &\\sim \\text{Normal}(\\mu_{\\beta}, \\sigma_{\\beta}) \\\\\n",
        "\\sigma &\\sim \\text{Half-Normal}(100) \\\\\n",
        "FVC_{ij} &\\sim \\text{Normal}(\\alpha_i + t \\beta_i, \\sigma)\n",
        "\\end{align}\n",
        "\n",
        "where *t* is the time in weeks. Those are very uninformative priors, but that's ok: our model will converge!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise 3.2\n",
        "1. Structurally (i.e. don't worry if you chose different priors for the $\\sigma$ terms, or made different (hyper)parameter choices), can you see how this model is a simple extension of the model from Exercise 1?\n",
        "\n",
        "2. Imagine the FVC data for every patient is identical. What would the model infer for each of the parameters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answers to Exercise 3.2\n",
        "1. Structurally (i.e. don't worry if you chose different priors for the $\\sigma$ terms, or made different (hyper)parameter choices), can you see how this model is a simple extension of the model from Exercise 1?\n",
        "\n",
        "**Answer**: Exercise 1.1 basically gives us the lower part of this generative story. We had\n",
        "\\begin{align}\n",
        "\\alpha &\\sim \\text{Normal}(\\mu_{\\alpha}, \\sigma_{\\alpha}) \\\\\n",
        "\\beta &\\sim \\text{Normal}(\\mu_{\\beta}, \\sigma_{\\beta}) \\\\\n",
        "\\sigma &\\sim \\text{Half-Normal}(100) \\\\\n",
        "FVC_{ij} &\\sim \\text{Normal}(\\alpha + t \\beta, \\sigma)\n",
        "\\end{align}\n",
        "The only difference now is that the parameters of all the distributions here are themselves drawn from another set of distibutions. We then allow for each patient to be a distinct sample from those distributions.\n",
        "\n",
        "**Note**: In principle there is nothing to stop you making the highest level (hyperparmaters) come from another set of distributions. In this sense Bayesian modelling has a bit of a \"Turtles all the way down\" situation going on.\n",
        "\n",
        "2. Imagine the FVC data for every patient is identical. What would the model infer for each of the parameters?\n",
        "\n",
        "**Answer**: The model should reduce back to vanilla Bayesian linear regression. More precisely\n",
        "* $\\alpha_{i} = \\alpha_{k} = \\mu_{\\alpha} $, $\\forall$ $i$, $k$, (where $i$, $k$ index the patients).\n",
        "* $\\beta_{i} = \\beta_{k} = \\mu_{\\beta}$ $\\forall$ $i$, $k$\n",
        "* $\\sigma_{\\alpha} = \\sigma_{\\beta} = 0$\n",
        "* The rest of the parameters will depend on the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing this model in NumPyro can be done as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model(patient_code, Weeks, FVC_obs=None):\n",
        "    # Prior distributions for global model parameters\n",
        "    μ_α = numpyro.sample(\"μ_α\", dist.Normal(0.0, 500.0))\n",
        "    σ_α = numpyro.sample(\"σ_α\", dist.HalfNormal(100.0))\n",
        "    μ_β = numpyro.sample(\"μ_β\", dist.Normal(0.0, 3.0))\n",
        "    σ_β = numpyro.sample(\"σ_β\", dist.HalfNormal(3.0))\n",
        "\n",
        "    # Calculate the number of unique patients\n",
        "    n_patients = len(np.unique(patient_code))\n",
        "\n",
        "    # Hierarchical model for patient-specific parameters (α and β)\n",
        "    with numpyro.plate(\"plate_i\", n_patients):\n",
        "        α = numpyro.sample(\"α\", dist.Normal(μ_α, σ_α))\n",
        "        β = numpyro.sample(\"β\", dist.Normal(μ_β, σ_β))\n",
        "\n",
        "    # Prior distribution for residual standard deviation\n",
        "    σ = numpyro.sample(\"σ\", dist.HalfNormal(100.0))\n",
        "\n",
        "    # Calculate the estimated FVC values for each patient and week\n",
        "    FVC_est = α[patient_code] + β[patient_code] * Weeks\n",
        "\n",
        "    # Likelihood model for observed FVC values\n",
        "    with numpyro.plate(\"data\", len(patient_code)):\n",
        "        numpyro.sample(\"obs\", dist.Normal(FVC_est, σ), obs=FVC_obs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise 3.3\n",
        "Comment every line of the model code above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's all for modelling!\n",
        "\n",
        "#### Fitting the model\n",
        "Before we perform inference, let's add a numerical Patient ID for each patient code. That can be done with scikit-learn's LabelEncoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "patient_encoder = LabelEncoder()\n",
        "train[\"patient_code\"] = patient_encoder.fit_transform(train[\"Patient\"].values)\n",
        "FVC_obs = train[\"FVC\"].values\n",
        "Weeks = train[\"Weeks\"].values\n",
        "patient_code = train[\"patient_code\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the model and training data defined, we can check the DAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numpyro.render_model(model, model_args=(patient_code,Weeks,FVC_obs,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now, we perform inference by calling NumPyro's inference engine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a NUTS (No-U-Turn Sampler) kernel for MCMC sampling\n",
        "nuts_kernel = NUTS(model)\n",
        "# Choose how many warm up, and actual samples we will take\n",
        "num_samples=2000\n",
        "num_warmup=2000\n",
        "# Create an MCMC (Markov Chain Monte Carlo) sampler with settings\n",
        "mcmc = MCMC(nuts_kernel, num_samples=num_samples, num_warmup=num_warmup)\n",
        "# Set a random number generator (RNG) key for reproducibility\n",
        "rng_key = random.PRNGKey(1)\n",
        "# Run the MCMC sampler to obtain posterior samples\n",
        "mcmc.run(rng_key, patient_code, Weeks, FVC_obs=FVC_obs)\n",
        "# Get the posterior samples from the MCMC run\n",
        "posterior_samples = mcmc.get_samples()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Further explanation of the above code for the dedicated reader:\n",
        "\n",
        "The \"kernel\" and the \"sampler\" are closely related components in the context of Markov Chain Monte Carlo (MCMC) algorithms, but they serve different roles in the MCMC process:\n",
        "\n",
        "Kernel: The kernel is responsible for proposing new states from the current state in the MCMC chain. It defines how the chain transitions from one state to another. The kernel specifies the proposal distribution and the acceptance criteria for new states.\n",
        "\n",
        "Sampler: The sampler manages the overall MCMC process, including the iteration over a sequence of states.\n",
        "It incorporates the kernel to generate new states, evaluate acceptance criteria, and update the state of the Markov chain.\n",
        "The sampler controls the number of iterations, warm-up (burn-in) phase, and final sampling phase. It collects and stores the samples drawn from the Markov chain, representing the posterior distribution of model parameters.\n",
        "\n",
        "In the code above, NUTS is the kernel that defines how the chain transitions between states, while MCMC is the sampler that manages the overall MCMC process and collects the posterior samples. The sampler uses the kernel as a component to generate the state transitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Checking the model\n",
        "##### Inspecting the learned parameters\n",
        "First, let's inspect the parameters learned. To do that, I will use [ArviZ](https://arviz-devs.github.io/arviz/), which integrates with NumPyro very nicely.\n",
        "\n",
        "The following plots are standard ways of \"eyeballing\" the learning process that occured using MCMC. On the left we have the inferred posterior distributions and on the right we have the trace plots.\n",
        "\n",
        "Trace plots are essential for assessing the convergence and mixing of the Markov Chain. There is one trace for each parameter in the model.\n",
        "\n",
        "Notes on Interpretation:\n",
        "* In general one looks for smooth and stable traces with minimal autocorrelation. A smooth trace means the chain has converged, and it's not stuck in one region.\n",
        "* Check for any visual patterns or trends in the traces. For well-converged chains, you should not see oscillations or wild swings.\n",
        "\n",
        "If this is your first time looking at a trace plot, then it might help to see some [examples of bad trace plots](https://jpreszler.rbind.io/post/2019-09-28-bad-traceplots/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import arviz as az\n",
        "data = az.from_numpyro(mcmc)\n",
        "az.plot_trace(data, compact=True, figsize=(15, 25))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main takeaway from these plots is that it looks like the model learned personalized alphas and betas for each patient!\n",
        "\n",
        "#### Visualizing FVC decline curves for some patients\n",
        "Now, let's visually inspect FVC decline curves predicted by our model. We will completely fill in the FVC table, predicting all missing values. These next few lines of code are not worth trying to follow in detail. **Feel free to skip to the plots!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prediction_template(unique_patient_df, weeks_series):\n",
        "    unique_patient_df[\"_temp\"] = True\n",
        "    weeks = pd.DataFrame(weeks_series, columns=[\"Weeks\"])\n",
        "    weeks[\"_temp\"] = True\n",
        "    return unique_patient_df.merge(weeks, on=\"_temp\").drop([\"_temp\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patients = train[[\"Patient\", \"patient_code\"]].drop_duplicates()\n",
        "start_week_number = -12\n",
        "end_week_number = 134\n",
        "predict_weeks = pd.Series(np.arange(start_week_number, end_week_number))\n",
        "pred_template = create_prediction_template(patients, predict_weeks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predicting the missing values in the FVC table and confidence (sigma) for each value becomes really easy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patient_code = pred_template[\"patient_code\"].values\n",
        "Weeks = pred_template[\"Weeks\"].values\n",
        "predictive = Predictive(model, posterior_samples, return_sites=[\"σ\", \"obs\"])\n",
        "samples_predictive = predictive(random.PRNGKey(0), patient_code, Weeks, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now put the predictions together with the true values, to visualize them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pred_template.copy()\n",
        "df[\"FVC_pred\"] = samples_predictive[\"obs\"].T.mean(axis=1)\n",
        "df[\"sigma\"] = samples_predictive[\"obs\"].T.std(axis=1)\n",
        "df[\"FVC_inf\"] = df[\"FVC_pred\"] - df[\"sigma\"]\n",
        "df[\"FVC_sup\"] = df[\"FVC_pred\"] + df[\"sigma\"]\n",
        "df = pd.merge(\n",
        "    df, train[[\"Patient\", \"Weeks\", \"FVC\"]], how=\"left\", on=[\"Patient\", \"Weeks\"]\n",
        ")\n",
        "df = df.rename(columns={\"FVC\": \"FVC_true\"})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's see our predictions for 3 patients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chart_patient_with_predictions(patient_id, ax):\n",
        "    data = df[df[\"Patient\"] == patient_id]\n",
        "    x = data[\"Weeks\"]\n",
        "    ax.set_title(patient_id)\n",
        "    ax.plot(x, data[\"FVC_true\"], \"o\")\n",
        "    ax.plot(x, data[\"FVC_pred\"])\n",
        "    ax = sns.regplot(x=x, y=data[\"FVC_true\"], ax=ax, ci=None, line_kws={\"color\": \"red\"})\n",
        "    ax.fill_between(x, data[\"FVC_inf\"], data[\"FVC_sup\"], alpha=0.5, color=\"#ffcd3c\")\n",
        "    ax.set_ylabel(\"FVC\")\n",
        "\n",
        "\n",
        "f, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "chart_patient_with_predictions(\"ID00007637202177411956430\", axes[0])\n",
        "chart_patient_with_predictions(\"ID00009637202177434476278\", axes[1])\n",
        "chart_patient_with_predictions(\"ID00011637202177653955184\", axes[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results are exactly what we expected to see! Highlight observations:\n",
        "\n",
        "* The model adequately learned Bayesian Linear Regressions! The orange line (learned predicted FVC mean) is very inline with the red line (deterministic linear regression). But most important: it learned to predict uncertainty, showed in the light orange region (one sigma above and below the mean FVC line).\n",
        "* The model predicts a higher uncertainty where the data is more scattered (1st and 3rd patients). Conversely, where the points are closely grouped together (2nd patient), the model predicts a higher confidence (narrower light orange region)\n",
        "* Finally, in all patients, we can see that the uncertainty grows as we extrapolate further from the data seen in training (later weeks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary for section 3: Hierarchical Modelling\n",
        "* Hierarchical modelling is a flexible approach for enabling information to be shared accross sub-models.\n",
        "* We looked at an example where the \"sub-models\" seek to predict the progression of a disease on an individual patient basis and made use of hierarchical modelling to enable information about features common to many patients to be shared between the models.\n",
        "* We started to discuss how one can assess if the model training went well or not but a proper treatment is of this is beyond the scope of this one-day course. We will provide suggestions for further reading which cover this topic in detail.\n",
        "* The example demonstrated a valuable property of Bayesian modelling, which is that these models naturally provide a measure of uncertainty. Modelling uncertainty is a very active area of research and viewed as critical to safe real world use of machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZbig3hnuuH3"
      },
      "source": [
        "# **Section 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK8_2S_XopGy"
      },
      "source": [
        "# Final Remarks\n",
        "\n",
        "Well done for making it to the end!!!\n",
        "\n",
        "## Next steps\n",
        "If you are interested in learning more about Bayesian modelling there are many amazing resources out there. Here is a list of resources and topics that you might want to look into in more detail if you are thinking about using these techniques in your own research.\n",
        "\n",
        "## Full notebook with answers\n",
        "\n",
        "You can find a completed and more detailed version of this notebook in the [github repo](https://github.com/DiasFrazerGroup/Intro-to-Bayesian-Modelling-Course).\n",
        "\n",
        "### Topics\n",
        "\n",
        "#### Big Topics\n",
        "* Bayesian workflow. There are many resources out there on this topic. [Here is a recent review](https://arxiv.org/abs/2011.01808). [There is also this humungous tutorial](https://arxiv.org/abs/2011.01808)\n",
        "* Bayesian Deep Learning (Bayesian variational autoencoders are a nice place to start)\n",
        "* Information Geometry\n",
        "* High-dimensional Probability\n",
        "* Robust Inference\n",
        "* Gaussian Processes\n",
        "* Variational Inference\n",
        "* Markov Chain Monte Carlo\n",
        "\n",
        "#### Smaller topics\n",
        "* Bayesian Model Comparison\n",
        "* [Non-centered models](https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/)\n",
        "* [Maximum entropy models](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution)\n",
        "* [The exponential family](https://en.wikipedia.org/wiki/Exponential_family)\n",
        "* [Jeffreys prior](https://en.wikipedia.org/wiki/Jeffreys_prior)\n",
        "* [Sparsity inducing priors](https://betanalpha.github.io/assets/case_studies/modeling_sparsity.html)\n",
        "* [Derivation of the ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound)\n",
        "\n",
        "### Books and lectures\n",
        "pdfs of these books are all freely available online\n",
        "* [Kevin Murphy - Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html)\n",
        "* [Kevin Murphy - Probabilistic Machine Learning: Advanced Topics](https://probml.github.io/pml-book/book2.html)\n",
        "* [Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin: Bayesian data analysis](https://stat.columbia.edu/~gelman/book/)\n",
        "* [Richard McElreath: Statistical Rethinking (2023 Edition)](https://github.com/rmcelreath/stat_rethinking_2023#calendar--topical-outline) and here are [all chapters implemented in Pyro](https://fehiepsi.github.io/rethinking-pyro/)\n",
        "* [Andrew Gordon Wilson - Bayesian Deep Learning and Probabilistic Model Construction - ICML 2020 Tutorial](https://youtu.be/E1qhGw8QxqY?si=fqeS48uFlCVP0Yul)\n",
        "* [Jaynes - Probability Theory: The Logic of Science](https://bayes.wustl.edu/etj/prob/book.pdf)\n",
        "\n",
        "### Some Inspiring People\n",
        "* [Andrew Gordon Wilson](https://cims.nyu.edu/~andrewgw/)\n",
        "* [Emtiyaz Khan](https://emtiyaz.github.io/)\n",
        "* [David Duvenaud](https://www.cs.toronto.edu/~duvenaud/)\n",
        "* [David Blei](https://www.cs.columbia.edu/~blei/)\n",
        "* [Jacob Steinhardt](https://jsteinhardt.stat.berkeley.edu/)\n",
        "* [Smita Krishnaswamy](https://krishnaswamylab.org/)\n",
        "* [Max Welling](https://staff.fnwi.uva.nl/m.welling/)\n",
        "* [Jennifer Listgarten](http://www.jennifer.listgarten.com/)\n",
        "* [Zoubin Ghahramani](https://mlg.eng.cam.ac.uk/zoubin/)\n",
        "* [Yee Whye Teh](https://www.stats.ox.ac.uk/~teh/)\n",
        "* [Tamara Broderick](https://tamarabroderick.com/)\n",
        "* [Jeff Miller](https://jwmi.github.io/)\n",
        "* [Debora Marks](https://www.deboramarkslab.com/)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
